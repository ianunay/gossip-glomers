# Gossip Glomers

Solutions to fly.io's distributed systems challenges - [https://fly.io/dist-sys/](https://fly.io/dist-sys/)

> The challenges are built on top of a platform called [Maelstrom](https://github.com/jepsen-io/maelstrom), which in turn, is built on Jepsen. This platform lets you build out a “node” in your distributed system and Maelstrom will handle the routing of messages between the those nodes. This lets Maelstrom inject failures and perform verification checks based on the consistency guarantees required by each challenge.

# Running the tests

Make sure `go` is installed.

Run `make test-echo`, `make test-unique-ids`, `make test-broadcast-single-node`, ... etc to build the binary and execute the maelstrom's test command

_check [Makefile](./Makefile) for the test commands and their definitions_

to see results on the web interface provided by Maelstrom run `make serve` and open [http://localhost:8080/](http://localhost:8080/)

## Challenge #1: Echo

[echo.go](./cmd/echo/echo.go)

As its the first problem, https://fly.io/dist-sys/1/ already explains all the details on how to handle STDIN message and respond the same message back on STDOUT

sequence diagram:

<img alt="image" src="https://github.com/ianunay/gossip-glomers/assets/1129363/afff83e3-d485-44d4-aa69-7b3936869ede">

## Challenge #2: Unique ID Generation

[unique-ids.go](./cmd/unique-ids/unique-ids.go)

The goal of the challenge is to make sure that id generated by the distributed system is globally unique. UUID should be sufficient to create unique ids without collision but to be extra safe node id is appended to the end. I'm not entirely sure if revealing node ids is a good idea probably encoding the node id using a dictionary together with the UUID could make the ids collision proof

sequence diagram:

<img alt="image" src="https://github.com/ianunay/gossip-glomers/assets/1129363/b769a725-1fb7-4f5a-a91d-5af2b9293624">

## Challenge #3: Single-Node Broadcast, Multi-Node Broadcast, Fault Tolerant Broadcast

[broadcast.go](./cmd/broadcast/broadcast.go)

In this challenge, step by step we build a fault tolerant multi node broadcast system.

A broadcast message is sent to a node n{x}, node n{x} then broadcasts the message to all other nodes using a topology (the solution above doesn't use topology sent by Jepsen test). This ensures data consistency between the nodes of a distributed system.

To make it fault tolerant during network partitions keep retrying until request is successful. Exponential backoff would be preferred to reduce congestions but it would mean the system cannot guarantee strong consistency.

sequence diagram:

<img alt="image" src="https://github.com/ianunay/gossip-glomers/assets/1129363/23acdf9d-8fae-4938-86d0-3d0d01246ac1">

### 3d: Efficient Broadcast, Part I; 3e: Efficient Broadcast, Part II

Haven't worked on these parts, the goal of is to achieve less gossip with the downside being that the latency is higher.

# Incomplete / Todo

There are a few bugs with the solutions below that need to be addressed

## Challenge #4: Grow-Only Counter

[grow-only-counter.go](./cmd/grow-only-counter/grow-only-counter.go)

On `add` store the delta in seq-kv (sequential key value store) with key being node id and value being the total sum of all delta's sent to the node.

On `read` fetch the value of total count stored in seq-kv for each node and sum them to get the final count.

sequence diagram:

<img alt="image" src="https://github.com/ianunay/gossip-glomers/assets/1129363/df696653-33c2-4b6f-adb7-7c4f54eff5ad">

## Challenge #5: Single-Node Kafka-Style Log, Multi-Node Kafka-Style Log, Efficient Kafka-Style Log

[kafka-log](./cmd/kafka-log/kafka-log.go)

Implement the following RPC's: send, poll, commit_offsets, list_committed_offsets

## Challenge #6a: Single-Node, Totally-Available Transactions

[txn-rw-register.go](./cmd/txn-rw-register/txn-rw-register.go)

Handle read & write transactions according to RPC definition
